# -*- coding: utf-8 -*-
"""wineQuality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BF6BFQYIUXfOPv4ENDiL-5tk0SziNiJu
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# Mục mới"""

data = pd.read_csv("winequality-red.csv",";")
df = pd.DataFrame(data)
df

plt.figure(figsize=(10, 6))
sns.countplot(data["quality"], palette="muted")

fig, ax =plt.subplots(figsize=(10,10))
dataplot = sns.heatmap (data.corr(), annot = True, ax=ax)

"""# Mục mới"""

df.info()

df

for i in range(len(df)):
  if(df['quality'][i] == 3):
    df['quality'][i] = 0
  elif(df['quality'][i] == 4):
    df['quality'][i] = 0
  elif(df['quality'][i] == 5):
    df['quality'][i] = 0
  elif(df['quality'][i] == 6):
    df['quality'][i] = 1
  elif(df['quality'][i] == 7):
    df['quality'][i] = 1
  elif(df['quality'][i] == 8):
    df['quality'][i] = 1

df

from sklearn.model_selection import GridSearchCV
def Parameter_tunning(x,y,models,clsModelsNm,parameters,score):
  tuned_params ={}
  for i,model in enumerate(models):
    print(clsModelsNm[i])
    grid = GridSearchCV(estimator = model,
                        cv=5,
                        param_grid = parameters[clsModelsNm[i]]
                        ,scoring = score
                        ,n_jobs = -1
                        )
  grid.fit(x,y)
  print(grid.best_score_)
  print(grid.best_params_)
  tuned_params[clsModelsNm[i]] = {'params':grid.best_params_}

  return tuned_params

plt.figure(figsize=(10, 6))
sns.countplot(df["quality"], palette="muted")

import sklearn
from sklearn.model_selection import GridSearchCV

Y = df['quality']
X = df.drop(columns=['quality'])

clf = tree.DecisionTreeClassifier()
parameters={"splitter":["best","random"],
            "max_depth" : [1,3,5,7,9,11,12],
           "min_samples_leaf":[1,2,3,4,5,6,7,8,9,10],
           "min_weight_fraction_leaf":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],
           "max_features":["auto","log2","sqrt",None],
           "max_leaf_nodes":[None,10,20,30,40,50,60,70,80,90] }
tuning_model=GridSearchCV(clf,param_grid=parameters,scoring='neg_mean_squared_error',cv=3,verbose=3)
tuning_model.fit(X,Y)
print(tuning_model.best_params_)

import sklearn
from sklearn import tree
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
kf = KFold(n_splits=10,shuffle=True,random_state=5)
result = 0
for i, (train, test) in enumerate(kf.split(df)):
  
  x_train = df.iloc[train]
  y_train = df.iloc[train]['quality']
  x_train = x_train.drop(columns=['quality'])
  x_test = df.iloc[test]
  y_test = df.iloc[test]['quality']
  x_test = x_test.drop(columns=['quality'])
  
  clf = tree.DecisionTreeClassifier(max_depth= 5, max_leaf_nodes= 90, min_samples_leaf= 6, min_weight_fraction_leaf= 0.1, splitter= 'best')
  clf = clf.fit(x_train, y_train)
 
  predict = clf.predict(x_test)
  y_test = y_test.tolist()
  re = 0
  for e in range(len(predict)):
    if predict[e] == y_test[e]:
      re +=1
  result +=re/len(predict)

result/=10
result

Parameter_tunning()